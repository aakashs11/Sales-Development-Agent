{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b8d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ca63a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation\n",
      "{'tool_use_correctness': False, 'tool_use_reason': \"The assistant used the 'run_primary_llm_query' tool correctly to retrieve leads from Bombay and to list people who work at universities. However, it failed to use the 'generate_sales_email' tool when the user requested to draft an email for lead 588124. Instead, it asked for additional information, which was not necessary as the tool should have been used.\", 'hallucination': False, 'hallucination_reason': \"The assistant did not provide any hallucinated information. All responses were based on the data retrieved from the tool or were within the scope of the assistant's knowledge.\", 'redundant_tool_call': False, 'redundant_tool_reason': 'The tool calls made by the assistant were necessary for retrieving the requested information about leads and universities.', 'out_of_scope': True, 'out_of_scope_reason': 'The assistant provided out-of-scope information when it answered the question about the capital of Maharashtra. This information is outside the context of assisting with sales leads and emails.'}\n",
      "Conversation\n",
      "{'tool_use_correctness': True, 'tool_use_reason': 'The assistant correctly asked the user how it could assist with sales development tasks, which is within its scope.', 'hallucination': False, 'hallucination_reason': \"The assistant did not provide any information that was not based on the user's input.\", 'redundant_tool_call': False, 'redundant_tool_reason': 'No tools were called, so there were no redundant tool calls.', 'out_of_scope': False, 'out_of_scope_reason': \"The assistant's response was within the scope of assisting with sales development tasks.\"}\n",
      "Conversation\n",
      "{'tool_use_correctness': True, 'tool_use_reason': \"The assistant correctly used the 'run_primary_llm_query' to fetch leads, 'generate_sales_email' to draft an email, and 'send_email' to send the email. It also correctly asked for missing information such as the lead's name and email address.\", 'hallucination': False, 'hallucination_reason': 'The assistant did not provide any information that was not supported by the data or context provided by the user.', 'redundant_tool_call': False, 'redundant_tool_reason': 'Each tool call was necessary for the task at hand, and there were no repeated or unnecessary calls.', 'out_of_scope': False, 'out_of_scope_reason': 'The assistant stayed within the scope of assisting with lead information, email drafting, and sending emails, which aligns with its defined role.'}\n",
      "Conversation\n",
      "{'tool_use_correctness': True, 'tool_use_reason': 'No tools were used or needed as the conversation is empty.', 'hallucination': False, 'hallucination_reason': 'No information was provided, so no hallucination occurred.', 'redundant_tool_call': False, 'redundant_tool_reason': 'No tool calls were made, so none could be redundant.', 'out_of_scope': False, 'out_of_scope_reason': 'No information was provided, so nothing could be out of scope.'}\n",
      "Conversation\n",
      "{'tool_use_correctness': True, 'tool_use_reason': \"The assistant correctly used the 'run_primary_llm_query' tool to find leads from the main database that come from 'Landing Page'. It also correctly asked for additional information from the user when attempting to draft an email.\", 'hallucination': False, 'hallucination_reason': 'The assistant did not provide any information that was not supported by the data returned from the tool calls.', 'redundant_tool_call': False, 'redundant_tool_reason': \"Each tool call was necessary to fulfill the user's requests for information about leads and their sources.\", 'out_of_scope': False, 'out_of_scope_reason': \"The assistant's responses were within the scope of assisting with lead analysis and email generation.\"}\n",
      "Conversation\n",
      "{'tool_use_correctness': True, 'tool_use_reason': \"The assistant correctly used the 'run_primary_llm_query' tool to fetch unconverted leads from Bombay and leads from the landing page. It also correctly used the 'generate_sales_email' tool to draft an email after gathering necessary information from the user.\", 'hallucination': False, 'hallucination_reason': 'The assistant did not provide any information that was not supported by the data retrieved from the tool calls.', 'redundant_tool_call': False, 'redundant_tool_reason': \"Each tool call was necessary to fulfill the user's requests, and there were no repeated or unnecessary tool calls.\", 'out_of_scope': False, 'out_of_scope_reason': 'The assistant stayed within the scope of its role by focusing on lead analysis and email generation, without providing unrelated information.'}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "evals = []\n",
    "client = OpenAI()\n",
    "folder = Path(os.getcwd()).parent / r\"app\\saved_conversations\"\n",
    "CONVERSATION_FILES = []\n",
    "def run_eval(folder: str) -> List[Dict[str, Any]]:\n",
    "\n",
    "    eval_prompt_base = \"\"\"\n",
    "    \n",
    "    You are evaluating a conversation between a Sales AI Assistant and a user.\n",
    "    Context : The Sales AI Assistant is designed to assist users in search and analysis of leads, generating sales emails and sending them. \n",
    "    The assistant will not provide any other information or perform any other tasks outside of this context.\n",
    "\n",
    "    #INPUT\n",
    "    Conversation:\n",
    "    {{conversation}}\n",
    "\n",
    "    # EVALUATION CRITERIA AND SCORING RUBRIC\n",
    "    Here are the evaluation criteria and the rubric that you need to use for evaluating the task:\n",
    "    <evaluation_criteria>\n",
    "    Check for correctness of tool usage, hallucinations, missing information handling, redundant tool calls, and out-of-scope responses.\n",
    "    </evaluation_criteria>\n",
    "    \n",
    "    <scoring_rubric>\n",
    "    True/False for each of the following:\n",
    "    Tools used in the conversation:\n",
    "    - run_primary_llm_query: \"run_primary_llm_query\" is a function that runs a query to the primary LLM. it requires a query.\n",
    "    - generate_sales_email: \"generate_sales_email\" is a function that generates a sales email. It requires lead number, product, first name, last name from the user.\n",
    "    - send_email: \"send_email\" is a function that sends an email. It requires subject, body, and recipient email address from the user.\n",
    "    - tool_use_correctness: Did the assistant use the correct tool for the task it was trying to accomplish? If yes, assign true. If no tools were needed, assign true. If assistant correctly asked for information from the user, assign true. If the assistant used a tool incorrectly, assign false.\n",
    "    - tool_use_reason: Explain why the tool usage is correct or incorrect.\n",
    "    - hallucination: Did the assistant provide any hallucinated information? If yes, assign true, otherwise false.\n",
    "    - hallucination_reason: Explain why the information is considered hallucinated or not.\n",
    "    - redundant_tool_call: Did the assistant make any redundant tool calls? If yes, assign true, otherwise false.\n",
    "    - redundant_tool_reason: Explain why the tool call is considered redundant or not.\n",
    "    - out_of_scope: Did the assistant provide any out-of-scope information in context of the role? If yes, assign true, otherwise false.\n",
    "    - out_of_scope_reason: Explain why the information is considered out-of-scope or not.\n",
    "    </scoring_rubric>\n",
    "\n",
    "    Below is a snippet of the conversation. Respond in the following JSON format:\n",
    "    STRICTLY follow the format and do not add any extra text like \"Here is the evaluation\" or \"The evaluation is\" or ```json```.\n",
    "    {{\n",
    "    \"tool_use_correctness\": true/false,\n",
    "    \"tool_use_reason\": \"...\",\n",
    "    \"hallucination\": true/false,\n",
    "    \"hallucination_reason\": \"...\",\n",
    "    \"redundant_tool_call\": true/false,\n",
    "    \"redundant_tool_reason\": \"...\"\n",
    "    \"out_of_scope\": true/false,\n",
    "    \"out_of_scope_reason\": \"...\",\n",
    "    }}\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    #CLEAN THE CONVERSATION FILES FOR EVALUATION\n",
    "    def clean_conversation(raw_convo: List[Dict]) -> List[Dict]:\n",
    "    \n",
    "\n",
    "        cleaned = []\n",
    "        #clean the conversation by removing system prompt\n",
    "        for msg in raw_convo[1:]:\n",
    "            entry = {\"role\": msg[\"role\"]}\n",
    "\n",
    "            # Handle content\n",
    "            entry[\"content\"] = msg.get(\"content\") or \"[No content]\"\n",
    "\n",
    "            # Handle tool_calls from assistant\n",
    "            if \"tool_calls\" in msg and msg[\"tool_calls\"]:\n",
    "                entry[\"tool_calls\"] = [\n",
    "                    {\n",
    "                        \"tool\": call.get(\"function\", {}).get(\"name\", \"\"),\n",
    "                        \"args\": call.get(\"function\", {}).get(\"arguments\", \"\")\n",
    "                    }\n",
    "                    for call in msg[\"tool_calls\"]\n",
    "                ]\n",
    "\n",
    "            # Handle function_call (legacy)\n",
    "            if \"function_call\" in msg and msg[\"function_call\"]:\n",
    "                entry[\"tool_calls\"] = [{\n",
    "                    \"tool\": msg[\"function_call\"].get(\"name\", \"\"),\n",
    "                    \"args\": msg[\"function_call\"].get(\"arguments\", \"\")\n",
    "                }]\n",
    "\n",
    "            # Handle tool output\n",
    "            if msg[\"role\"] == \"tool\":\n",
    "                entry[\"tool_name\"] = msg.get(\"name\", \"\")\n",
    "                entry[\"tool_output\"] = msg.get(\"content\", \"\")\n",
    "\n",
    "            cleaned.append(entry)\n",
    "        return cleaned\n",
    "    \n",
    "    #LOAD CONVERSATION FILES\n",
    "    def load_conversation_files() :\n",
    "        for file in os.listdir(folder):\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = folder / file\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                    data = clean_conversation(data)\n",
    "                    CONVERSATION_FILES.append(data)\n",
    "                    \n",
    "    load_conversation_files()\n",
    "\n",
    "    for conversation in CONVERSATION_FILES:\n",
    "        print(\"Conversation\")\n",
    "        # Prepare the evaluation prompt with the cleaned conversation\n",
    "        eval_prompt = eval_prompt_base.replace(\"{{conversation}}\", json.dumps(conversation))\n",
    "\n",
    "        # Prepare the messages for the API call\n",
    "    \n",
    "        messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert evaluator.\"},\n",
    "                    {\"role\": \"user\", \"content\": eval_prompt},\n",
    "                ]\n",
    "        response = client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=\"gpt-4o\",\n",
    "            stream=False,\n",
    "            temperature=0,\n",
    "\n",
    "        )\n",
    "\n",
    "        response_content = response.choices[0].message.content\n",
    "        # Parse the response content to extract the JSON data\n",
    "        try:\n",
    "            response_json = json.loads(response_content)\n",
    "            evals.append(response_json)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Failed to parse JSON response for conversation:\")\n",
    "            return None\n",
    "        print(response_json)\n",
    "run_eval(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5c5e8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Aggregate Evaluation Metrics:\n",
      "total_evaluations: 6\n",
      "tool_use_correct_percentage: 83.33\n",
      "hallucination_percentage: 0.0\n",
      "redundant_tool_call_percentage: 0.0\n",
      "out_of_scope_percentage: 16.67\n",
      "missing_info_handling_breakdown: {'not_applicable': 6}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "def aggregate_metrics(evals):\n",
    "    total = len(evals)\n",
    "    if total == 0:\n",
    "        print(\"No evaluations found!\")\n",
    "        return {}\n",
    "    \n",
    "    # Compute counts of True values from boolean fields\n",
    "    tool_use_correct_count = sum(1 for ev in evals if ev.get(\"tool_use_correctness\", False))\n",
    "    hallucination_count = sum(1 for ev in evals if ev.get(\"hallucination\", False))\n",
    "    redundant_tool_call_count = sum(1 for ev in evals if ev.get(\"redundant_tool_call\", False))\n",
    "    out_of_scope_count = sum(1 for ev in evals if ev.get(\"out_of_scope\", False))\n",
    "    \n",
    "    # Count the different responses for missing_info_handling (\"good\", \"bad\", \"not_applicable\")\n",
    "    missing_info_counter = Counter(ev.get(\"missing_info_handling\", \"not_applicable\") for ev in evals)\n",
    "    \n",
    "    metrics = {\n",
    "        \"total_evaluations\": total,\n",
    "        \"tool_use_correct_percentage\": round(tool_use_correct_count / total * 100, 2),\n",
    "        \"hallucination_percentage\": round(hallucination_count / total * 100, 2),\n",
    "        \"redundant_tool_call_percentage\": round(redundant_tool_call_count / total * 100, 2),\n",
    "        \"out_of_scope_percentage\": round(out_of_scope_count / total * 100, 2),\n",
    "        \"missing_info_handling_breakdown\": dict(missing_info_counter)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "agg_metrics = aggregate_metrics(evals)\n",
    "print(\"Final Aggregate Evaluation Metrics:\")\n",
    "for key, value in agg_metrics.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a4e82e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
